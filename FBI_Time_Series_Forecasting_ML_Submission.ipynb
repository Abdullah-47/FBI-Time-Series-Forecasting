{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "nA9Y7ga8ng1Z",
        "bKJF3rekwFvQ",
        "GF8Ens_Soomf",
        "g-ATYxFrGrvw",
        "-7MS06SUHkB-",
        "tEA2Xm5dHt1r",
        "hwyV_J3ipUZe",
        "xiyOF9F70UgQ",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "P1XJ9OREExlT",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "-Kee-DAl2viO"
      ],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - FBI Time Series Forecasting\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Time Series Forecasting\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "FBI crime data analysis involves studying trends and patterns in reported crime statistics collected by the Federal Bureau of Investigation (FBI).\n",
        "\n",
        "*   **Crime Trends:** Identifying how crime rates are changing over time, both nationally and regionally. This can help understand whether certain types of crime are increasing or decreasing.\n",
        "*   **Geographical Distribution:** Examining where crime is more prevalent and understanding potential contributing factors in those areas.\n",
        "*   **Seasonal Patterns:** Recognizing if certain crimes are more common during specific times of the year.\n",
        "*   **Relationship with Socioeconomic Factors:** Investigating potential correlations between crime rates and factors like poverty, unemployment, education levels, and population density.\n",
        "*   **Effectiveness of Crime Prevention Strategies:** While correlation doesn't equal causation, analyzing data before and after the implementation of specific programs can provide some insights into their potential impact.\n",
        "*   **Resource Allocation:** Helping law enforcement agencies and policymakers make informed decisions about where to allocate resources to combat crime effectively.\n",
        "*   **Forecasting:** Using historical data to predict future crime trends, which can aid in proactive policing and resource planning.\n",
        "\n",
        "This project aims to perform `time series forecasting` on FBI crime data to predict future crime trends. This can be valuable for law enforcement agencies, policymakers, and researchers to anticipate future challenges, allocate resources effectively. The project will likely involve:\n",
        "\n",
        "1.  **Data Acquisition and Cleaning:** Obtaining relevant FBI crime data and preparing it for analysis (handling missing values, inconsistencies, etc.).\n",
        "2.  **Exploratory Data Analysis (EDA):** Analyzing the data to identify trends, seasonality, and other patterns.\n",
        "3.  **Time Series Model Selection:** Choosing appropriate time series forecasting models (e.g., ARIMA, Exponential Smoothing, Prophet, or machine learning models).\n",
        "4.  **Model Training and Evaluation:** Training the selected models on historical data and evaluating their performance using appropriate metrics.\n",
        "5.  **Forecasting:** Using the trained model to predict future crime rates.\n",
        "6.  **Interpretation and Reporting:** Interpreting the forecasting results and presenting the findings in a clear and actionable manner."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Analyze the given FBI Crime Dataset (Training and Testing) and perform Time Series Forecasting to it.\n",
        "- Perform Data visualization, Data wrangling, Feature Engineering, Data Splitting, Model Fitting, Hyperparameter Tuning.\n",
        "- Make prediction on unseen data. Predict `Incident_count` based on `Year`,`Month`,`Type`(Type of crime commited eg, Theft)"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Datasets\n",
        "!pip install category_encoders\n",
        "!pip install sklearn\n",
        "!pip install statsmodels"
      ],
      "metadata": {
        "id": "lqJVnuG02n7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import datetime\n",
        "import warnings\n",
        "import itertools\n",
        "import category_encoders as ce\n",
        "from datasets import load_dataset\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import chi2\n",
        "from scipy.stats.mstats import winsorize\n",
        "from scipy.linalg import inv\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from pandas.plotting import scatter_matrix\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.formula.api import rlm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Load the training data\n",
        "try:\n",
        "    df_train = pd.read_csv(\"https://huggingface.co/datasets/Abdullah4747/FBI_Train/resolve/main/Train.csv\")\n",
        "    print(\"Training data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Train.csv' not found.\")\n",
        "    df_train = pd.DataFrame()  # Initialize an empty DataFrame to avoid further errors\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading 'Train.xlsx': {e}\")\n",
        "    df_train = pd.DataFrame()\n",
        "\n",
        "# Load the test data\n",
        "try:\n",
        "    df_test = pd.read_csv(\"https://huggingface.co/datasets/Abdullah4747/FBI_Train/resolve/main/Test%20(2).csv\")\n",
        "    print(\"Test data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Test (2).csv' not found.\")\n",
        "    df_test = pd.DataFrame()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading 'Test (2).csv': {e}\")\n",
        "    df_test = pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Display and verify the loaded data\n",
        "if not df_train.empty:\n",
        "    display(df_train.head())\n",
        "\n",
        "if not df_test.empty:\n",
        "    display(df_test.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "if not df_train.empty:\n",
        "    print(f\"Shape of df_train: {df_train.shape}\")\n",
        "if not df_test.empty:\n",
        "    print(f\"Shape of df_test: {df_test.shape}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "if not df_train.empty:\n",
        "    print(\"Information about df_train:\")\n",
        "    df_train.info()\n",
        "if not df_test.empty:\n",
        "    print(\"\\nInformation about df_test:\")\n",
        "    df_test.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values and Unique Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "if not df_train.empty:\n",
        "    print(f\"Number of duplicate values in df_train: {df_train.duplicated().sum()}\")\n",
        "    print(f\"Number of unique values in df_train: {df_train.nunique()}\")\n",
        "# Display duplicate rows if any\n",
        "if df_train.duplicated().sum() > 0:\n",
        "    print(\"\\nDuplicate rows in df_train:\")\n",
        "    display(df_train[df_train.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "if not df_train.empty:\n",
        "    print(\"Missing values in df_train:\")\n",
        "    print(df_train.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "if not df_train.empty:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df_train.isnull(), cbar=False, cmap='viridis').set_title(\"Missing Values Heatmap\")"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Information\n",
        "\n",
        "### Training Data (`df_train`)\n",
        "- **Size**: 474,565 incidents √ó 13 features\n",
        "- **Time Span**: 13 years (2003-2015)  \n",
        "- **Key Features**:\n",
        "  - Crime categories (`TYPE`: 9 unique)\n",
        "  - Geographic details (`NEIGHBOURHOOD`: 24 areas)\n",
        "  - Daily timestamps (`Date`: 4,748 unique dates)\n",
        "  - Spatial coordinates (fully populated)\n",
        "- **Data Types**:\n",
        "  - Float64: 6 coordinate columns\n",
        "  - Integer: 3 date components\n",
        "  - Object: 4 categorical features\n",
        "- **Memory Usage**: 47.1+ MB\n",
        "\n",
        "### Test Data (`df_test`)\n",
        "- **Size**: 162 forecast points √ó 4 columns\n",
        "- **Forecast Task**:\n",
        "  - Predict **monthly crime counts** for 9 crime types\n",
        "  - Horizon: **18 months** (Jan 2016 - Jun 2017)\n",
        "- **Structure**:\n",
        "  - Features: `YEAR`, `MONTH`, `TYPE`\n",
        "  - Target: `Incident_Counts` (to be predicted)\n",
        "\n",
        "---\n",
        "\n",
        "## Duplicate Values\n",
        "1. Total **44,618** duplicate rows (9.4% of data)\n",
        "2. **Implications**:\n",
        "   - Indicates possible data entry errors or genuine recurring incidents\n",
        "   - May skew forecast if not addressed\n",
        "3. **Action Required**: Remove duplicates before analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Unique Values\n",
        "| Column          | Unique Values | Significance                          |\n",
        "|-----------------|---------------|---------------------------------------|\n",
        "| `TYPE`          | 9             | Valid crime categories                |\n",
        "| `NEIGHBOURHOOD` | 24            | Expected neighborhood divisions       |\n",
        "| `Date`          | 4,748         | Confirms ‚âà13 years of daily data      |\n",
        "| `HOUR`/`MINUTE` | 24/60         | Proper time granularity               |\n",
        "\n",
        "**Key Takeaway**: Unique counts validate data integrity with no unexpected categories/ranges\n",
        "\n",
        "---\n",
        "\n",
        "## Missing Values\n",
        "1. **Significant Missingness** (>10%):\n",
        "   - `NEIGHBOURHOOD`: 51,491 missing (10.8%)\n",
        "   - `HOUR`/`MINUTE`: 49,365 missing each (10.4%)\n",
        "2. **Minor Missingness**:\n",
        "   - `HUNDRED_BLOCK`: 13 missing (0.003%)\n",
        "3. **Key Observations**:\n",
        "   - Missing `HOUR`/`MINUTE` share same rows (time unrecorded)\n",
        "   - Location/time gaps limit granular analysis\n",
        "   - **Critical forecasting columns** (`YEAR`, `MONTH`, `TYPE`, `Date`) fully intact\n",
        "4. **Forecasting Impact**:\n",
        "   - No effect on monthly aggregated forecasts\n",
        "   - Neighborhood-level analysis would require imputation"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "if not df_train.empty:\n",
        "    print(\"Columns in df_train:\")\n",
        "    print(df_train.columns)\n",
        "if not df_test.empty:\n",
        "    print(\"\\nColumns in df_test:\")\n",
        "    print(df_test.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "if not df_train.empty:\n",
        "    print(\"Summary statistics for df_train:\")\n",
        "    display(df_train.describe())\n",
        "if not df_test.empty:\n",
        "    print(\"\\nSummary statistics for df_test:\")\n",
        "    display(df_test.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary Statistics\n",
        "\n",
        "### Training Data (`df_train`)\n",
        "\n",
        "| Statistic | X             | Y               | Latitude    | Longitude   | HOUR     | MINUTE    | YEAR     | MONTH    | DAY      |\n",
        "|-----------|---------------|-----------------|-------------|-------------|----------|-----------|----------|----------|----------|\n",
        "| Count     | 474,565       | 474,565         | 474,565     | 474,565     | 425,200  | 425,200   | 474,565  | 474,565  | 474,565  |\n",
        "| Mean      | 441,028.02    | 4,889,023.00    | 44.14       | -110.30     | 13.72    | 16.74     | 2004.36  | 6.56     | 15.44    |\n",
        "| Std Dev   | 150,295.32    | 1,665,850.00    | 15.04       | 37.58       | 6.79     | 18.35     | 3.85     | 3.41     | 8.76     |\n",
        "| Min       | **0.00**      | **0.00**        | **0.00**    | -124.55     | 0.00     | 0.00      | 1999     | 1        | 1        |\n",
        "| 25%       | 489,916.53    | 5,453,572.00    | 49.23       | -123.13     | 9.00     | 0.00      | 2001     | 4        | 8        |\n",
        "| 50%       | 491,477.85    | 5,456,820.00    | 49.26       | -123.11     | 15.00    | 10.00     | 2004     | 7        | 15       |\n",
        "| 75%       | 493,610.19    | 5,458,622.00    | 49.28       | -123.07     | 19.00    | 30.00     | 2008     | 9        | 23       |\n",
        "| Max       | 511,303.00    | 5,512,579.00    | 49.76       | **0.00**    | 23.00    | 59.00     | 2011     | 12       | 31       |\n",
        "\n",
        "**Key Observations:**\n",
        "1. ‚ö†Ô∏è **Spatial Data Issues**:\n",
        "   - `Latitude` minimum (0.00) and `Longitude` maximum (0.00) are invalid for crime locations\n",
        "   - Extreme minimums in `X` and `Y` (0.00) suggest measurement errors\n",
        "2. ‚è∞ **Temporal Patterns**:\n",
        "   - Peak crime hours: 3PM (median HOUR=15)\n",
        "   - Most crimes occur mid-month (median DAY=15)\n",
        "   - Summer peak (median MONTH=7, July)\n",
        "3. üìÖ **Date Range**: 1999-2011 (13 years)\n",
        "\n",
        "---\n",
        "\n",
        "### Test Data (`df_test`)\n",
        "\n",
        "| Statistic | YEAR    | MONTH   | Incident_Counts |\n",
        "|-----------|---------|---------|-----------------|\n",
        "| Count     | 162     | 162     | 0               |\n",
        "| Mean      | 2012.33 | 5.50    | NaN             |\n",
        "| Std Dev   | 0.47    | 3.31    | NaN             |\n",
        "| Min       | 2012    | 1       | NaN             |\n",
        "| 25%       | 2012    | 3       | NaN             |\n",
        "| 50%       | 2012    | 5       | NaN             |\n",
        "| 75%       | 2013    | 8       | NaN             |\n",
        "| Max       | 2013    | 12      | NaN             |\n",
        "\n",
        "**Forecast Task Specifications:**\n",
        "- **Prediction Period**: January 2012 - June 2013 (18 months)\n",
        "- **Monthly Distribution**:\n",
        "  - Balanced coverage (median=May, mean=June)\n",
        "  - Full calendar cycles included\n",
        "- **Target**: `Incident_Counts` to be predicted for all 162 records\n",
        "\n",
        "---\n",
        "\n",
        "### Critical Data Issues\n",
        "\n",
        "| Issue Type               | Location         | Impact  | Recommended Action                     |\n",
        "|--------------------------|------------------|---------|----------------------------------------|\n",
        "| Invalid Coordinates      | Lat=0, Lon=0     | High    | Remove or impute spatial outliers      |\n",
        "| Extreme Values           | X/Y min=0        | Medium  | Investigate measurement errors         |\n",
        "| Train-Test Gap           | 2011 ‚Üí 2012      | Critical| Validate on 2009-2011 data             |\n",
        "| Missing Temporal Data    | HOUR/MINUTE 10.4%| Low     | Ignore for monthly aggregation         |"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "def preprocess_data(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Preprocess FBI crime datasets for time series forecasting\n",
        "    Handles duplicates, missing values, invalid data, and prepares for monthly aggregation\n",
        "    \"\"\"\n",
        "    # 1. Handle duplicates\n",
        "    train_df = train_df.drop_duplicates()\n",
        "\n",
        "    # 2. Handle missing values\n",
        "    # Critical columns are intact, so we'll keep missing temporal/spatial data for monthly aggregation\n",
        "    # Drop HUNDRED_BLOCK with minimal missingness\n",
        "    train_df = train_df.drop(columns=['HUNDRED_BLOCK'], errors='ignore')\n",
        "\n",
        "    # 3. Fix invalid spatial data\n",
        "    spatial_mask = ((train_df['Latitude'] == 0) | (train_df['Longitude'] == 0) |\n",
        "                   (train_df['X'] == 0) | (train_df['Y'] == 0))\n",
        "    train_df = train_df[~spatial_mask]\n",
        "\n",
        "    # 4. Convert to datetime and extract features\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'], format='%d/%m/%Y')\n",
        "    train_df['Year'] = train_df['Date'].dt.year\n",
        "    train_df['Month'] = train_df['Date'].dt.month\n",
        "\n",
        "    # 5. Verify date ranges\n",
        "    print(f\"Train date range: {train_df['Date'].min().date()} to {train_df['Date'].max().date()}\")\n",
        "    print(f\"Test date range: {test_df['YEAR'].min()}-{test_df['MONTH'].min()} to {test_df['YEAR'].max()}-{test_df['MONTH'].max()}\")\n",
        "\n",
        "    # 6. Aggregate to monthly crime counts\n",
        "    monthly_counts = train_df.groupby(['Year', 'Month', 'TYPE']).size().reset_index(name='Incident_Counts')\n",
        "\n",
        "    # 7. Prepare test set structure\n",
        "    test_df = test_df.rename(columns={'MONTH': 'Month', 'YEAR': 'Year'})\n",
        "\n",
        "    return monthly_counts, test_df\n",
        "\n",
        "# Execute preprocessing\n",
        "train_monthly, test_df = preprocess_data(df_train, df_test)\n",
        "\n",
        "# Display processed data\n",
        "print(\"\\nProcessed Training Data:\")\n",
        "print(train_monthly.head())\n",
        "print(f\"\\nTraining shape: {train_monthly.shape}\")\n",
        "\n",
        "print(\"\\nTest Data:\")\n",
        "print(test_df.head())\n",
        "print(f\"\\nTest shape: {test_df.shape}\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Wrangling Summary\n",
        "\n",
        "### Key Manipulations Performed\n",
        "\n",
        "#### 1. **Data Quality Cleaning**\n",
        "\n",
        "- **Duplicate Removal**: Eliminated duplicate records from training data\n",
        "\n",
        "- **Invalid Spatial Data**: Removed records with invalid coordinates (Lat=0, Lon=0, X=0, Y=0)\n",
        "\n",
        "- **Column Cleanup**: Dropped `HUNDRED_BLOCK` column due to minimal impact and missingness\n",
        "\n",
        "#### 2. **Temporal Processing**\n",
        "\n",
        "- **Date Standardization**: Converted date strings (`DD/MM/YYYY`) to proper datetime format\n",
        "\n",
        "- **Feature Extraction**: Created `Year` and `Month` columns from date field\n",
        "\n",
        "- **Date Range Verification**: Confirmed training spans 1999-2011, test covers 2012-2013\n",
        "\n",
        "#### 3. **Data Aggregation**\n",
        "\n",
        "- **Monthly Summarization**: Transformed individual incident records into monthly crime counts by type\n",
        "\n",
        "- **Grouping Strategy**: Aggregated by `[Year, Month, TYPE]` to create time series structure\n",
        "\n",
        "- **Target Creation**: Generated `Incident_Counts` as the prediction target\n",
        "\n",
        "#### 4. **Test Set Preparation**\n",
        "\n",
        "- **Column Standardization**: Renamed `YEAR`‚Üí`Year`, `MONTH`‚Üí`Month` for consistency\n",
        "\n",
        "- **Structure Alignment**: Maintained same schema as processed training data\n",
        "\n",
        "---\n",
        "\n",
        "### Key Insights Obtained\n",
        "\n",
        "#### üìä **Data Transformation Impact**\n",
        "\n",
        "- **Scale Reduction**: From 474,565 individual incidents ‚Üí 1,248 monthly aggregates\n",
        "\n",
        "- **Dimensionality**: Focused analysis on temporal patterns rather than spatial/individual incident details\n",
        "\n",
        "- **Data Quality**: Eliminated ~10% of records due to invalid spatial coordinates\n",
        "\n",
        "#### üïí **Temporal Structure**\n",
        "\n",
        "- **Training Period**: 13 years (1999-2011) of historical crime data\n",
        "\n",
        "- **Prediction Gap**: 1-month gap between training end (Dec 2011) and test start (Jan 2012)\n",
        "\n",
        "- **Forecast Horizon**: 24 months of predictions required (2012-2013)\n",
        "\n",
        "#### üîç **Crime Type Distribution**\n",
        "\n",
        "- **Dominant Categories**:\n",
        "\n",
        "¬† - Break and Enter (Commercial & Residential)\n",
        "\n",
        "¬† - Theft from Vehicle (highest single category: 1,438 incidents in Jan 1999)\n",
        "\n",
        "¬† - Mischief and Other Theft\n",
        "\n",
        "- **Monthly Variability**: Each crime type shows different seasonal patterns\n",
        "\n",
        "#### ‚ö†Ô∏è **Modeling Considerations**\n",
        "\n",
        "- **Missing Targets**: All 162 test records have `NaN` incident counts (expected for prediction task)\n",
        "\n",
        "- **Time Series Setup**: Monthly aggregation enables seasonal trend analysis\n",
        "\n",
        "- **Multi-series Forecasting**: Need to predict multiple crime types simultaneously\n",
        "\n",
        "This preprocessing successfully transformed raw incident data into a clean, time-series ready format suitable for monthly crime forecasting across different offense categories."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Univariate Line Plot of Total Incident Counts Over Time"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# 1. Create a datetime index and aggregate total incidents by date\n",
        "train_monthly['date'] = pd.to_datetime(train_monthly[['Year', 'Month']].assign(DAY=1))\n",
        "df_agg = train_monthly.groupby('date')['Incident_Counts'].sum().reset_index()\n",
        "\n",
        "# 2. Plot the time series\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(df_agg['date'], df_agg['Incident_Counts'])\n",
        "plt.title('Total Incident Counts (Monthly) from Jan 1999 to Dec 2011')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Incident Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Line plot provides a clear view of the overall trend and seasonality"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trend: The line exhibits a generally downward slope, we can infer that the total crime incidents  fell over the training period. For instance, a downward trend from 2000-2009 followed by a upward trend from 2009‚Äì2012 suggests changing underlying risk factors.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Overall downward trend signals that existing intervention programs (like community policing) are effective which justifies continued investment."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Seasonal Box Plot of Monthly Incident Counts"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# 1. Reuse df_agg from Chart 1, extract month\n",
        "df_agg['month'] = df_agg['date'].dt.month\n",
        "\n",
        "# 2. Prepare data as list of series for each month (Jan=1 ‚Ä¶ Dec=12)\n",
        "monthly_groups = [df_agg.loc[df_agg['month'] == m, 'Incident_Counts'] for m in range(1, 13)]\n",
        "\n",
        "# 3. Plot box plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.boxplot(monthly_groups, labels=[\n",
        "    'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
        "])\n",
        "plt.title('Distribution of Total Incidents by Month (1999‚Äì2011)')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Incident Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examines seasonality; shows spread, outliers, medians for each calendar month."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì July, August, September have higher medians and upper quartiles, meaning summer is riskier.\n",
        "‚Äì April, May show outliers/spikes (could be one-time events)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Schedules summer policing, targets spring outlier months for deeper review."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Bar Chart of Overall Incident Counts by TYPE"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# 1. Aggregate total incidents by TYPE across all years\n",
        "counts_by_type = train_monthly.groupby('TYPE')['Incident_Counts'].sum().sort_values(ascending=False)\n",
        "\n",
        "# 2. Plot bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(counts_by_type.index, counts_by_type.values)\n",
        "plt.title('Total Incident Counts by Crime TYPE (1999‚Äì2011)')\n",
        "plt.xlabel('Crime TYPE')\n",
        "plt.ylabel('Total Incident Counts')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highlights which crime types have highest cumulative volume."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì \"Theft from Vehicle\" is dominant, followed by Mischief and B&E.\n",
        "‚Äì Last two types (Bicycle Theft, Collision with Injury) are relatively rare."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Positive: Justifies resource allocation to car theft/break-ins."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Univariate Heatmap of Year vs. Month"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# 1. Pivot table: index=Year, columns=Month, values=sum of Incident_Counts\n",
        "pivot_year_month = train_monthly.pivot_table(\n",
        "    index='Year',\n",
        "    columns='Month',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "# 2. Plot heatmap using imshow\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(pivot_year_month, aspect='auto', origin='lower')\n",
        "plt.colorbar(label='Incident Counts')\n",
        "plt.title('Heatmap of Incident Counts by Year (1999‚Äì2011) and Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Year')\n",
        "plt.xticks(ticks=range(0, 12), labels=[\n",
        "    'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
        "])\n",
        "plt.yticks(ticks=range(len(pivot_year_month.index)), labels=pivot_year_month.index)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Compactly shows both seasonality and historical evolution in one chart."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì 1999‚Äì2002 were consistently higher, especially May‚ÄìOct; post-2007 every month is cooler.\n",
        "‚Äì 2011 sees a ‚Äúhot‚Äù winter compared to previous years."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Positive: Directs trend- and season-based forecasting; flags years (ex. 1999‚Äì2002) needing review."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Rolling Mean & Rolling Standard Deviation Plot (12-Month Window)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# 1. Compute 12-month rolling mean and std on the aggregated series\n",
        "df_agg.set_index('date', inplace=True)\n",
        "rolling_mean = df_agg['Incident_Counts'].rolling(window=12).mean()\n",
        "rolling_std = df_agg['Incident_Counts'].rolling(window=12).std()\n",
        "\n",
        "# 2. Plot both on the same figure\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(df_agg.index, df_agg['Incident_Counts'], label='Original', alpha=0.4)\n",
        "plt.plot(rolling_mean.index, rolling_mean, label='12-Month Rolling Mean')\n",
        "plt.plot(rolling_std.index, rolling_std, label='12-Month Rolling Std', linestyle='--')\n",
        "plt.title('Rolling Mean & Std Dev of Total Incidents (12-Month Window)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Incident Counts / Std Dev')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Restore df_agg index if needed later:\n",
        "df_agg.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Measures moving average (trend) and volatility; checks for stationarity."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì STD remained relatively stable; Mean dropped until 2008 then rebounded mildly post-2010.\n",
        "‚Äì Indicates volatility didn‚Äôt shrink as much as mean, implying relative unpredictability at low levels."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Positive: Highlights when volatility is low (resource-saving months)."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Univariate Autocorrelation Function (ACF) Plot of Incident Counts"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "\n",
        "# 1. Ensure df_agg has date index\n",
        "df_agg.set_index('date', inplace=True)\n",
        "\n",
        "# 2. Plot ACF\n",
        "plt.figure(figsize=(6, 4))\n",
        "autocorrelation_plot(df_agg['Incident_Counts'])\n",
        "plt.title('Autocorrelation Plot of Total Incident Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Restore df_agg index if needed later:\n",
        "df_agg.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Quantifies persistence/memory and seasonality; essential for ARIMA modeling."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Strong autocorrelation (decays slowly), suggesting momentum; evidence of yearly periodicity."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Positive: Enables robust forecasting; identifies that last month/year is a strong predictor."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Bivariate Scatter Plot: ‚ÄúTheft from Vehicle‚Äù vs. ‚ÄúOther Theft‚Äù (Monthly)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# 1. Filter only the two TYPEs, aggregate monthly\n",
        "types_of_interest = ['Theft from Vehicle', 'Other Theft']\n",
        "df_two_types = train_monthly[train_monthly['TYPE'].isin(types_of_interest)].copy()\n",
        "df_two_types['date'] = pd.to_datetime(df_two_types[['Year', 'Month']].assign(DAY=1))\n",
        "df_pivot_two = df_two_types.pivot_table(\n",
        "    index='date',\n",
        "    columns='TYPE',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ").dropna()\n",
        "\n",
        "# 2. Scatter plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(\n",
        "    df_pivot_two['Theft from Vehicle'],\n",
        "    df_pivot_two['Other Theft'],\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.title('\"Theft from Vehicle\" vs. \"Other Theft\" (Monthly)')\n",
        "plt.xlabel('Monthly Theft from Vehicle')\n",
        "plt.ylabel('Monthly Other Theft')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Explores bivariate correlation of two key types, assessing co-movement."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Evidence of (likely weak to moderate) positive correlation for some range, but possibly nonlinear (clusters)."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Positive: Suggests resource allocation could jointly target these crimes."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Correlation Heatmap Among Top 5 Crime Types"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# 1. Identify top 5 TYPEs by total count\n",
        "top_5_types = counts_by_type.head(5).index.tolist()\n",
        "\n",
        "# 2. Pivot monthly counts for those TYPEs\n",
        "df_top5 = train_monthly[train_monthly['TYPE'].isin(top_5_types)].copy()\n",
        "df_top5['date'] = pd.to_datetime(df_top5[['Year', 'Month']].assign(DAY=1))\n",
        "pivot_top5 = df_top5.pivot_table(\n",
        "    index='date',\n",
        "    columns='TYPE',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "# 3. Compute correlation matrix\n",
        "corr_top5 = pivot_top5.corr()\n",
        "\n",
        "# 4. Plot correlation heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(corr_top5, vmin=-1, vmax=1, cmap='RdBu', origin='lower')\n",
        "plt.colorbar(label='Pearson Correlation')\n",
        "plt.xticks(ticks=range(len(top_5_types)), labels=top_5_types, rotation=45, ha='right')\n",
        "plt.yticks(ticks=range(len(top_5_types)), labels=top_5_types)\n",
        "plt.title('Correlation Matrix of Top 5 Crime Types')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Summarizes all mutual correlations in one glance."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì \"Theft from Vehicle\" and \"Theft of Vehicle\" are highly related; \"Break and Enter\" is more independent."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Äì Positive: Supports clustering crimes for joint interventions."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Multivariate Stacked Area Chart of Incident Counts by TYPE Over Time"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# 1. Pivot a DataFrame: index=date, columns=TYPE, values=monthly Incident_Counts\n",
        "df_all_types = train_monthly.copy()\n",
        "df_all_types['date'] = pd.to_datetime(df_all_types[['Year', 'Month']].assign(DAY=1))\n",
        "pivot_all = df_all_types.pivot_table(\n",
        "    index='date',\n",
        "    columns='TYPE',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "# 2. Sort TYPE columns by total count (descending)\n",
        "type_order = pivot_all.sum().sort_values(ascending=False).index\n",
        "pivot_all = pivot_all[type_order]\n",
        "\n",
        "# 3. Plot stacked area\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.stackplot(pivot_all.index, pivot_all.T, labels=type_order)\n",
        "plt.legend(loc='upper left', fontsize='small')\n",
        "plt.title('Stacked Area Chart: Monthly Incident Counts by TYPE (1999‚Äì2011)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Incident Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Shows composition and trends of all crime types simultaneously over time\n",
        "* Effective for visualizing both individual category performance and total volume\n",
        "* Good for identifying dominant crime categories and seasonal patterns\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Overall crime declined significantly  from ~4,000 incidents/month (1999-2000) to ~2,000-2,500 (2005-2011)\n",
        "*\"Theft from Vehicle\" (blue) dominates, representing ~40-50% of all incidents\n",
        "* Clear seasonal patterns with peaks in summer months\n",
        "* \"Break and Enter Residential/Other\" shows steady decline over time"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Crime reduction trend indicates effective policing strategies"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Bivariate Time Series Overlay (Dual Axes): ‚ÄúTheft from Vehicle‚Äù & ‚ÄúMischief‚Äù"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# 1. Select and pivot the two series\n",
        "series_two = train_monthly[train_monthly['TYPE'].isin(['Theft from Vehicle', 'Mischief'])].copy()\n",
        "series_two['date'] = pd.to_datetime(series_two[['Year', 'Month']].assign(DAY=1))\n",
        "pivot_two_ts = series_two.pivot_table(\n",
        "    index='date',\n",
        "    columns='TYPE',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "# 2. Plot on dual axes\n",
        "fig, ax1 = plt.subplots(figsize=(10, 4))\n",
        "ax1.plot(pivot_two_ts.index, pivot_two_ts['Theft from Vehicle'], label='Theft from Vehicle')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Theft from Vehicle', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(pivot_two_ts.index, pivot_two_ts['Mischief'], label='Mischief', linestyle='--')\n",
        "ax2.set_ylabel('Mischief', color='orange')\n",
        "ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "plt.title('Monthly \"Theft from Vehicle\" vs. \"Mischief\"')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compares two major crime categories with dual y-axes\n",
        "- Shows relationship between related crime types"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Theft from Vehicle peaked around 1999-2000 (~1,800/month) and declined to ~600-800/month\n",
        "- Both crimes show seasonal patterns but different magnitudes\n",
        "- Mischief remains relatively stable (~300-500/month) with less dramatic decline"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Positive: Significant reduction in vehicle theft suggests successful prevention programs\n",
        "- Negative: Mischief crimes haven't declined as dramatically, requiring focused intervention"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Box Plot of Incident Counts by Year (All Types Combined)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# 1. Use df_agg from Chart 1 (aggregate per month), extract year\n",
        "df_agg_box = df_agg.copy()\n",
        "df_agg_box['year'] = df_agg_box['date'].dt.year\n",
        "\n",
        "# 2. Prepare data: list of series for each year\n",
        "years = sorted(df_agg_box['year'].unique())\n",
        "data_by_year = [df_agg_box.loc[df_agg_box['year'] == y, 'Incident_Counts'] for y in years]\n",
        "\n",
        "# 3. Plot box plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.boxplot(data_by_year, labels=years)\n",
        "plt.title('Monthly Incident Counts Distribution by Year (1999‚Äì2011)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Incident Counts')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Shows distribution, median, and outliers for each year\n",
        "- Excellent for identifying variability and unusual periods"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Median incident counts dropped from ~3,800 (1999) to ~2,200 (2009)\n",
        "- High variability in early years (1999-2002) with wide boxes and outliers\n",
        "- More stable, predictable crime patterns in recent years (smaller boxes)"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: More predictable crime patterns allow better resource planning"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Violin Plot of Incident Counts by Quarter"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# 1. Add quarter label to df_agg\n",
        "df_agg_quarter = df_agg.copy()\n",
        "df_agg_quarter['quarter'] = df_agg_quarter['date'].dt.quarter\n",
        "\n",
        "# 2. Group incident counts by quarter\n",
        "quarters = [1, 2, 3, 4]\n",
        "data_by_quarter = [df_agg_quarter.loc[df_agg_quarter['quarter'] == q, 'Incident_Counts'] for q in quarters]\n",
        "\n",
        "# 3. Plot violin plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.violinplot(data_by_quarter, showmeans=True)\n",
        "plt.xticks(ticks=[1, 2, 3, 4], labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
        "plt.title('Violin Plot of Total Incidents by Quarter (1999‚Äì2011)')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Incident Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Shows seasonal distribution patterns across all years\n",
        "- Better than box plots for showing distribution shape"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Q2 (summer) shows highest incident peaks with widest distribution\n",
        "- Q1 has lowest median but high variability\n",
        "- Seasonal crime patterns are consistent across the 13-year period"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resource Planning: Allocate more officers during Q2 summer months"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Facet Grid of Time Series (One Subplot per TYPE)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# 1. Get unique TYPEs and set up subplots (e.g., 3 columns, n rows as needed)\n",
        "unique_types = df_train['TYPE'].unique()\n",
        "n_types = len(unique_types)\n",
        "n_cols = 3\n",
        "n_rows = (n_types + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "# 2. For each TYPE, plot its monthly series\n",
        "for idx, crime_type in enumerate(unique_types):\n",
        "    df_type = train_monthly[train_monthly['TYPE'] == crime_type].copy()\n",
        "    df_type['date'] = pd.to_datetime(df_type[['Year', 'Month']].assign(DAY=1))\n",
        "    series_type = df_type.groupby('date')['Incident_Counts'].sum().reset_index()\n",
        "\n",
        "    axes[idx].plot(series_type['date'], series_type['Incident_Counts'])\n",
        "    axes[idx].set_title(crime_type)\n",
        "    axes[idx].set_ylabel('Count')\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Hide any unused subplots\n",
        "for j in range(idx + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Shows detailed trends for each crime category\n",
        "- Enables comparison of different trajectory patterns"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vehicle theft shows dramatic decline (1,800 to 600)\n",
        "- Other Theft shows upward trend (concerning)\n",
        "- Break and Enter categories both declining\n",
        "- Theft of Bicycle remains relatively stable but low volume"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mixed Results: Success in some categories, concern in others\n",
        "- Resource Reallocation: Shift focus from declining categories to emerging problems"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Correlation Heatmap for Top 5 Crime Types\n",
        "\n",
        "# 1. Identify top 5 TYPEs by total count\n",
        "counts_by_type = train_monthly.groupby('TYPE')['Incident_Counts'].sum().sort_values(ascending=False)\n",
        "top_5_types = counts_by_type.head(5).index.tolist()\n",
        "\n",
        "# 2. Pivot monthly counts for those TYPEs\n",
        "df_top5 = train_monthly[train_monthly['TYPE'].isin(top_5_types)].copy()\n",
        "df_top5['date'] = pd.to_datetime(df_top5[['Year', 'Month']].assign(DAY=1))\n",
        "pivot_top5 = df_top5.pivot_table(\n",
        "    index='date',\n",
        "    columns='TYPE',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "# 3. Compute correlation matrix\n",
        "corr_top5 = pivot_top5.corr()\n",
        "\n",
        "# 4. Plot correlation heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(corr_top5.values, vmin=-1, vmax=1, origin='lower')\n",
        "plt.colorbar(label='Pearson Correlation')\n",
        "plt.xticks(ticks=range(len(top_5_types)), labels=top_5_types, rotation=45, ha='right')\n",
        "plt.yticks(ticks=range(len(top_5_types)), labels=top_5_types)\n",
        "plt.title('Correlation Matrix of Top 5 Crime Types')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Identifies relationships between different crime types\n",
        "- Helps understand if crimes are related or independent"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Strong positive correlation between \"Theft from Vehicle\" and \"Other Theft\"\n",
        "- Negative correlation between \"Break and Enter\" and other categories\n",
        "- Some crime types move independently (near-zero correlations)"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Pair Plot (Scatter Matrix) for Top 5 Crime Types\n",
        "\n",
        "# 1. Reuse pivot_top5 from above (monthly counts for top 5 types)\n",
        "#    If pivot_top5 is not already in memory, recreate it as shown in the heatmap code.\n",
        "\n",
        "# 2. Use scatter_matrix to create pairwise scatter plots + histograms on the diagonal\n",
        "plt.figure(figsize=(8, 8))\n",
        "scatter_matrix(\n",
        "    pivot_top5,\n",
        "    diagonal='hist',\n",
        "    alpha=0.5,\n",
        "    figsize=(8, 8)\n",
        ")\n",
        "plt.suptitle('Pair Plot: Monthly Counts for Top 5 Crime Types', y=0.92)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Comprehensive view of relationships between all crime pairs\n",
        "- Shows both distributions and correlations"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Confirms correlation findings from matrix\n",
        "- Shows clustering patterns in crime relationships\n",
        "- Identifies outlier months with unusual crime combinations"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis 1: Seasonal Crime Pattern\n",
        "Statement: \"Summer months (June, July, August) have significantly higher total crime incidents compared to winter months (December, January, February).\"\n",
        "\n",
        "---\n",
        "## Hypothesis 2: Crime Trend Structural Break\n",
        "Statement: \"There was a significant structural break in the crime trend around 2009, with crime rates declining from 1999-2009 and then reversing to an increasing trend from 2010-2011.\"\n",
        "\n",
        "---\n",
        "## Hypothesis 3: Vehicle Crime Correlation\n",
        "Statement: \"Theft from Vehicle and Theft of Vehicle incidents are significantly positively correlated, with changes in one crime type predicting changes in the other.\"\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H‚ÇÄ):¬†There is no significant difference in mean monthly crime incidents between summer months (June, July, August) and winter months (December, January, February).\\\n",
        "Œº_summer = Œº_winter\n",
        "\n",
        "Alternative Hypothesis (H‚ÇÅ):¬†Summer months have significantly higher mean monthly crime incidents than winter months.\\\n",
        "Œº_summer > Œº_winter (one-tailed test)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "# Extract incident counts for summer months (June=6, July=7, August=8)\n",
        "summer_incidents = train_monthly[train_monthly['Month'].isin([6, 7, 8])]['Incident_Counts']\n",
        "\n",
        "# Extract incident counts for winter months (December=12, January=1, February=2)\n",
        "winter_incidents = train_monthly[train_monthly['Month'].isin([12, 1, 2])]['Incident_Counts']\n",
        "\n",
        "# Perform an independent two-sample t-test (one-tailed)\n",
        "# The alternative='greater' specifies a one-tailed test where we check if the mean of the first sample (summer)\n",
        "# is greater than the mean of the second sample (winter).\n",
        "t_statistic, p_value = stats.ttest_ind(summer_incidents, winter_incidents, alternative='greater')\n",
        "\n",
        "print(f\"T-statistic: {t_statistic}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent two-sample t-test (one-tailed)\n",
        "Specifically, a one-tailed test with alternative='greater' to test if summer incidents > winter incidents."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-test is appropriate for continuous data with reasonable sample size"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H‚ÇÄ):¬†There is no structural break in the crime trend at 2009; the trend remains consistent throughout 1999-2011.\\\n",
        "Œ≤_before2009 = Œ≤_after2009\n",
        "\n",
        "Alternative Hypothesis (H‚ÇÅ):¬†There is a significant structural break in 2009, with different trend coefficients before and after this point.\\\n",
        "Œ≤_before2009 ‚â† Œ≤_after2009"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# 1. Prepare data for regression\n",
        "# Ensure df_agg has date column as datetime and extract year and month\n",
        "# The 'date' column already exists from previous steps, ensure it's datetime\n",
        "df_agg['date'] = pd.to_datetime(df_agg['date'])\n",
        "\n",
        "# Extract year and month from the existing 'date' column\n",
        "df_agg['year'] = df_agg['date'].dt.year\n",
        "df_agg['month'] = df_agg['date'].dt.month\n",
        "\n",
        "# Create a continuous time variable (e.g., month number since the start)\n",
        "df_agg['time'] = (df_agg['date'] - df_agg['date'].min()).dt.days\n",
        "\n",
        "# Create a dummy variable for the period after 2009 (starting from January 2009)\n",
        "df_agg['after_2008'] = (df_agg['year'] >= 2009).astype(int)\n",
        "\n",
        "# Create an interaction term: time * after_2008\n",
        "df_agg['time_after_2008'] = df_agg['time'] * df_agg['after_2008']\n",
        "\n",
        "# Define the dependent variable (Incident Counts) and independent variables\n",
        "Y = df_agg['Incident_Counts']\n",
        "X = df_agg[['time', 'after_2008', 'time_after_2008']]\n",
        "\n",
        "# Add a constant for the intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# 2. Fit the linear regression model\n",
        "model = sm.OLS(Y, X).fit()\n",
        "\n",
        "# 3. Obtain the p-value for the interaction term (time_after_2008)\n",
        "# The p-value for this term tests the null hypothesis that the trend coefficient\n",
        "# is the same before and after 2009 (i.e., the difference in slopes is zero).\n",
        "p_value = model.pvalues['time_after_2008']\n",
        "\n",
        "print(model.summary())\n",
        "print(f\"P-value for structural break in 2009: {p_value}\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear Regression with Interaction Term (t-test on coefficient)\n",
        "Specifically, a t-test on the time_after_2008 interaction term coefficient in an OLS regression model.\n",
        "- This is a common way to implement a structural break test like a Chow test when you have a specific breakpoint in mind."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It allows for the inclusion of a specific breakpoint (2009) in the time series model.\n",
        "- The interaction term directly tests for a significant difference in the trend slope before and after the breakpoint, which aligns with the alternative hypothesis (Œ≤_before2009 ‚â† Œ≤_after2009)."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H‚ÇÄ):¬†There is no significant correlation between \"Theft from Vehicle\" and \"Theft of Vehicle\" incidents.\\\n",
        "œÅ = 0\n",
        "\n",
        "Alternative Hypothesis (H‚ÇÅ):¬†There is a significant positive correlation between \"Theft from Vehicle\" and \"Theft of Vehicle\" incidents.\\\n",
        "œÅ > 0 (one-tailed test)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# 1. Extract the monthly incident counts for the two crime types\n",
        "#    Use the pivot_top5 DataFrame created earlier in Chart 8 or Chart 14\n",
        "#    Ensure pivot_top5 contains the columns 'Theft from Vehicle' and 'Theft of Vehicle'\n",
        "\n",
        "# If pivot_top5 is not available, recreate it:\n",
        "types_for_corr = ['Theft from Vehicle', 'Theft of Vehicle']\n",
        "df_corr = train_monthly[train_monthly['TYPE'].isin(types_for_corr)].copy()\n",
        "df_corr['date'] = pd.to_datetime(df_corr[['Year', 'Month']].assign(DAY=1))\n",
        "pivot_corr = df_corr.pivot_table(\n",
        "    index='date',\n",
        "    columns='TYPE',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "theft_from_vehicle = pivot_corr['Theft from Vehicle']\n",
        "theft_of_vehicle = pivot_corr['Theft of Vehicle']\n",
        "\n",
        "# 2. Perform the Pearson correlation test\n",
        "# pearsonr returns the Pearson correlation coefficient and a two-tailed p-value\n",
        "correlation_coefficient, two_tailed_p_value = pearsonr(theft_from_vehicle, theft_of_vehicle)\n",
        "\n",
        "# 3. Convert the two-tailed p-value to a one-tailed p-value\n",
        "# Since we are testing if correlation > 0 (one-tailed alternative),\n",
        "# and pearsonr gives a two-tailed p-value, we divide by 2.\n",
        "# This is valid if the calculated correlation coefficient is positive,\n",
        "# which we expect based on the H1. If it were negative, the one-tailed p-value\n",
        "# would be 1 - (two_tailed_p_value / 2).\n",
        "if correlation_coefficient > 0:\n",
        "    one_tailed_p_value = two_tailed_p_value / 2\n",
        "else:\n",
        "    # This case is unlikely given the previous charts, but included for completeness\n",
        "    one_tailed_p_value = 1 - (two_tailed_p_value / 2)\n",
        "\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {correlation_coefficient}\")\n",
        "print(f\"One-tailed P-value: {one_tailed_p_value}\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I have performed a Pearson correlation test to obtain the correlation coefficient and its corresponding p-value. Then, I converted the obtained two-tailed p-value to a one-tailed p-value to match the alternative hypothesis."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It is the standard statistical test for measuring the linear relationship (correlation) between two continuous variables.\n",
        "- It is appropriate for this data, which consists of monthly counts of incidents, treated as continuous variables for the purpose of this test."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "#No Missing values in training Data"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There are no Missing values in training data after i cleaned and aggregated it"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Winsorization\n",
        "\n",
        "# Assuming summer_incidents and winter_incidents are defined elsewhere\n",
        "# based on your seasonal analysis.\n",
        "# For example:\n",
        "# summer_months = [6, 7, 8]\n",
        "# winter_months = [12, 1, 2]\n",
        "# df_agg['month'] = df_agg['date'].dt.month # Make sure month is available\n",
        "# summer_incidents = df_agg[df_agg['month'].isin(summer_months)]['Incident_Counts']\n",
        "# winter_incidents = df_agg[df_agg['month'].isin(winter_months)]['Incident_Counts']\n",
        "\n",
        "# Check if summer_incidents and winter_incidents are available and not empty before winsorizing\n",
        "if 'summer_incidents' in globals() and 'winter_incidents' in globals() and not summer_incidents.empty and not winter_incidents.empty:\n",
        "    # Winsorize top 5% of values in both groups\n",
        "    summer_winsorized = winsorize(summer_incidents, limits=[0, 0.05])\n",
        "    winter_winsorized = winsorize(winter_incidents, limits=[0, 0.05])\n",
        "    print(\"Winsorization applied.\")\n",
        "else:\n",
        "    print(\"Could not perform Winsorization: summer_incidents or winter_incidents not defined or empty.\")\n",
        "\n",
        "\n",
        "#2 Robust regression with Huber loss\n",
        "# Assuming df_agg is defined and contains 'Incident_Counts', 'time', and 'after_2008'\n",
        "# For example, if time is a numerical representation of date and after_2008 is a dummy variable:\n",
        "# df_agg['time'] = np.arange(len(df_agg))\n",
        "# df_agg['after_2008'] = (df_agg['date'].dt.year > 2008).astype(int)\n",
        "\n",
        "# Check if df_agg is available and contains necessary columns before fitting RLM\n",
        "if 'df_agg' in globals() and not df_agg.empty and all(col in df_agg.columns for col in ['Incident_Counts', 'time', 'after_2008']):\n",
        "    robust_model = rlm('Incident_Counts ~ time * after_2008', data=df_agg).fit()\n",
        "    print(\"Robust regression model fitted.\")\n",
        "else:\n",
        "     print(\"Could not fit Robust Regression: df_agg not defined or missing columns 'Incident_Counts', 'time', 'after_2008'.\")\n",
        "\n",
        "\n",
        "#3 Mahalanobis Distance\n",
        "\n",
        "# Filter for the specific crime types and pivot to get them as columns\n",
        "vehicle_crime_types = ['Theft from Vehicle', 'Theft of Vehicle']\n",
        "vehicle_data_filtered = train_monthly[train_monthly['TYPE'].isin(vehicle_crime_types)].copy()\n",
        "\n",
        "# Pivot to have 'Theft from Vehicle' and 'Theft of Vehicle' as columns\n",
        "# We need to make sure that for each month/year combination, we have both types,\n",
        "# otherwise mahalanobis distance calculation will fail on different row counts or missing values.\n",
        "# If a type is missing for a given month/year, we fill it with 0.\n",
        "vehicle_data_pivot = vehicle_data_filtered.pivot_table(\n",
        "    index=['Year', 'Month'],\n",
        "    columns='TYPE',\n",
        "    values='Incident_Counts',\n",
        "    aggfunc='sum'\n",
        ").fillna(0) # Fill missing combinations with 0 incident counts\n",
        "\n",
        "\n",
        "# Select the columns for Mahalanobis distance calculation\n",
        "# Ensure the column names match the pivoted dataframe columns\n",
        "if 'Theft from Vehicle' in vehicle_data_pivot.columns and 'Theft of Vehicle' in vehicle_data_pivot.columns:\n",
        "    vehicle_data_for_maha = vehicle_data_pivot[['Theft from Vehicle', 'Theft of Vehicle']]\n",
        "\n",
        "    # Calculate covariance matrix and its inverse\n",
        "    cov = np.cov(vehicle_data_for_maha.values.T)\n",
        "\n",
        "    # Check if covariance matrix is singular\n",
        "    if np.linalg.det(cov) == 0:\n",
        "        print(\"Covariance matrix is singular. Cannot compute Mahalanobis distance.\")\n",
        "    else:\n",
        "        inv_cov = inv(cov)\n",
        "        mean = vehicle_data_for_maha.mean().values\n",
        "\n",
        "        # Calculate Mahalanobis distance for each row\n",
        "        mahalanobis_distances = [mahalanobis(row, mean, inv_cov)\n",
        "                              for row in vehicle_data_for_maha.values]\n",
        "\n",
        "        # Add Mahalanobis distance to the pivoted dataframe\n",
        "        vehicle_data_pivot['mahalanobis'] = mahalanobis_distances\n",
        "\n",
        "        # Remove outliers (œá¬≤ cutoff, Œ±=0.01)\n",
        "        # The degrees of freedom for chi2.ppf should be the number of variables used in Mahalanobis distance (2 in this case)\n",
        "        cutoff = np.sqrt(chi2.ppf(0.99, df=2))\n",
        "        cleaned_vehicle_data = vehicle_data_pivot[vehicle_data_pivot['mahalanobis'] <= cutoff]\n",
        "\n",
        "        print(\"\\nMahalanobis Distance Calculation:\")\n",
        "        print(f\"Number of data points before outlier removal: {len(vehicle_data_pivot)}\")\n",
        "        print(f\"Mahalanobis distance cutoff (sqrt(chi2.ppf(0.99, 2))): {cutoff:.4f}\")\n",
        "        print(f\"Number of data points after outlier removal: {len(cleaned_vehicle_data)}\")\n",
        "        print(\"Mahalanobis distance calculated and outliers removed.\")\n",
        "        print(cleaned_vehicle_data.head())\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Could not perform Mahalanobis Distance calculation: Pivoted dataframe does not contain expected columns ('Theft from Vehicle', 'Theft of Vehicle').\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the outlier treatments are done based upon the results of hypothesis testing.\n",
        "### Hypothesis 1 - Seasonal Crime Pattern\n",
        "- Original Result: Failed to reject H‚ÇÄ (p=0.074)\n",
        "\n",
        "- Outlier Treatment: `Winsorization (5% upper tail)`\n",
        "\n",
        "Why Used:\n",
        "\n",
        "- Borderline non-significance suggested possible outlier influence\n",
        "\n",
        "- Winsorization preserves sample size while reducing extreme value impact\n",
        "\n",
        "- Focused on upper tail (crime spikes) as these disproportionately affect means\n",
        "\n",
        "### Hypothesis 2: Crime Trend Structural Break\n",
        "- Original Result: Rejected H‚ÇÄ (p=4.24e-19)\n",
        "\n",
        "- Outlier Treatment: `Robust Regression (Huber Loss)`\n",
        "\n",
        "Why Used:\n",
        "\n",
        "- Extreme significance but model showed large condition number (88,400)\n",
        "\n",
        "- Protects against influential observations without removing data points\n",
        "\n",
        "- Maintains time-series continuity while downweighting outliers\n",
        "\n",
        "### Hypothesis 3: Vehicle Crime Correlation\n",
        "- Original Result: Rejected H‚ÇÄ (p=4.70e-59)\n",
        "\n",
        "- Outlier Treatment: `Mahalanobis Distance` (Œ±=0.01)\n",
        "\n",
        "Why Used:\n",
        "\n",
        "- Detects multivariate outliers in correlated crime types\n",
        "\n",
        "- œá¬≤ cutoff (3.0349) ensures only extreme joint deviations removed\n",
        "\n",
        "- Conservative: Removed only 3/156 points (1.9% of data)"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Manipulation\n",
        "# Create datetime index and temporal features\n",
        "train_monthly['date'] = pd.to_datetime(train_monthly[['Year', 'Month']].assign(day=1))\n",
        "train_monthly.set_index('date', inplace=True)\n",
        "\n",
        "# Create structural break features (post-2008 indicator)\n",
        "train_monthly['after_2008'] = (train_monthly.index.year >= 2009).astype(int)\n",
        "train_monthly['time'] = (train_monthly.index - train_monthly.index.min()).days\n",
        "train_monthly['time_after_2008'] = train_monthly['time'] * train_monthly['after_2008']\n",
        "\n",
        "# Feature Selection - Retain only essential columns\n",
        "selected_features = ['Incident_Counts', 'TYPE', 'time', 'after_2008', 'time_after_2008']\n",
        "train_monthly = train_monthly[selected_features]"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature selection is done in the feature manipulation section\n",
        "#and data wrangling section (Dropping columns and aggregation)."
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Manual column Dropping: The `HUNDRED_BLOCK` column is explicitly dropped during the data preprocessing step because it was not a critical feature for the forecasting task.\n",
        "\n",
        "2. Aggregation: The invidiual incident records are aggregated into monthly counts by `Year`,`Month`, and `TYPE`. This is to convert the granular incident data into time-series format as the key grouping features for the target variable   `Incident_counts`.\n",
        "\n",
        "3. Implicit Selection: By aggregating to monthly counts, features like `X`, `Y`, `Latitude`, `Longitude`, `HOUR`, and `MINUTE` are implicitly excluded. Reasoning is that they are not directly relevant for a monthly crime count time series forecast."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Year and Month: These are the `fundamental time components` that define the time series.\n",
        "\n",
        "2. TYPE: The problem requires forecasting monthly counts for `each crime type`.\n",
        "\n",
        "3. Incident_Counts (as the target variable):While not a feature used to predict, the historical Incident_Counts themselves are the most important information for time series forecasting."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode crime types\n",
        "le = LabelEncoder()\n",
        "train_monthly['TYPE_encoded'] = le.fit_transform(train_monthly['TYPE'])\n",
        "\n",
        "# Log transformation for variance stabilization\n",
        "train_monthly['log_incidents'] = np.log1p(train_monthly['Incident_Counts'])\n",
        "\n",
        "# Create differenced features for stationarity\n",
        "train_monthly['diff_1'] = train_monthly.groupby('TYPE')['Incident_Counts'].diff(1)\n",
        "train_monthly['diff_12'] = train_monthly.groupby('TYPE')['Incident_Counts'].diff(12)\n",
        "\n",
        "# Drop initial rows with NaNs from differencing\n",
        "train_monthly = train_monthly.dropna(subset=['diff_1', 'diff_12'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes the data needed to be transformed because the raw data contained too much unnecessary detail `exact time`, `location` for a monthly forecasting task and forecasting models require data in `time-series` format\n",
        "- Ways in which the data is transformed:\n",
        "- `Aggregation` to Monthly Counts: The goal is to forecast monthly crime counts. The original data is at the individual incident level, which is too granular for this forecasting task.\n",
        "- `Datetime` Index: Time series analysis relies on a proper temporal index to order data and identify patterns like seasonality and trends. Creating a date column from Year and Month allows for time-based operations. This was done using `pd.to_datetime` in the `preprocess_data `function.\n",
        "\n"
      ],
      "metadata": {
        "id": "dQtlHo7rYgjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time series models (SARIMA, Exponential Smoothing) are scale-invariant. Scaling is `unnecessary` as they rely on relative changes rather than absolute values."
      ],
      "metadata": {
        "id": "zjptCzNg6gZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features are already low-dimensional post-aggregation (5‚Äì7 features). Dimensionality reduction could lose interpretability of crime-type-specific patterns. Hence Dimensionality reduction is `not needed`"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is not necessary so it is `not applied`."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Chronological split - Last year (2011) as validation\n",
        "train_end = '2010-12-31'\n",
        "val_start = '2011-01-01'\n",
        "\n",
        "train_data = train_monthly[train_monthly.index <= train_end]\n",
        "val_data = train_monthly[train_monthly.index >= val_start]\n",
        "\n",
        "# Prepare final feature/target sets\n",
        "X_train = train_data[['TYPE_encoded', 'time', 'after_2008', 'time_after_2008']]\n",
        "y_train = train_data['Incident_Counts']\n",
        "\n",
        "X_val = val_data[['TYPE_encoded', 'time', 'after_2008', 'time_after_2008']]\n",
        "y_val = val_data['Incident_Counts']\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training Data: Covers the period from `1999 to 2010`.\n",
        "- Validation Data: Covers the period from `2011 to 2012`\n",
        "- Test Data: Covers the period from `2012 to 2013`.\n",
        "- Reasoning: A `chronological split` ensures that the model is trained only on data that occurred before the period it needs to forecast. The problem statement and test data structure explicitly define the forecasting task as predicting monthly crime counts for 2012-2013. `Validation` set is used to evaluate the performance of the forecast model."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly crime counts are continuous values, `not classes`. Imbalance handling (e.g., SMOTE) is irrelevant for regression-style forecasting."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`No need` because the dataset is in continous values not classes."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_crimes = [\n",
        "    'Break and Enter Commercial',\n",
        "    'Break and Enter Residential/Other',\n",
        "    'Mischief',\n",
        "    'Other Theft',\n",
        "    'Theft from Vehicle',\n",
        "    'Theft of Bicycle',\n",
        "    'Theft of Vehicle',\n",
        "    'Vehicle Collision or Pedestrian Struck (with Injury)',\n",
        "    'Offence Against a Person'\n",
        "]\n",
        "# Define crime-specific configurations\n",
        "crime_config = {\n",
        "    'Theft from Vehicle': {\n",
        "        'transform': 'log',\n",
        "        'd': [2],  # Force 2nd-order differencing\n",
        "        'q': [2, 3],  # Higher MA terms\n",
        "        'error_metric': 'smape'  # Use symmetric MAPE\n",
        "    },\n",
        "    'Theft of Bicycle': {\n",
        "        'transform': 'log',\n",
        "        'd': [1, 2],\n",
        "        'error_metric': 'smape'\n",
        "    },\n",
        "    'default': {\n",
        "        'transform': None,\n",
        "        'error_metric': 'mae'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Symmetric MAPE function\n",
        "def smape(actual, forecast):\n",
        "    return 200 * np.mean(np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
        "\n",
        "# Prepare data structures\n",
        "crime_forecasts = {}\n",
        "validation_metrics = {}\n",
        "final_model=None #Declare final_model as global\n",
        "for crime_type in all_crimes:\n",
        "    print(f\"\\n===== Modeling: {crime_type} =====\")\n",
        "\n",
        "    # Skip if no data\n",
        "    if crime_type not in train_monthly['TYPE'].unique():\n",
        "        print(f\"Skipping {crime_type} - no data\")\n",
        "        continue\n",
        "\n",
        "    # Get config\n",
        "    config = crime_config.get(crime_type, crime_config['default'])\n",
        "\n",
        "    # Extract series\n",
        "    raw_series = train_monthly[train_monthly['TYPE'] == crime_type]['Incident_Counts']\n",
        "\n",
        "    # Apply transformation\n",
        "    if config['transform'] == 'log':\n",
        "        series = np.log1p(raw_series)\n",
        "        print(\"Applied log transformation\")\n",
        "    else:\n",
        "        series = raw_series.copy()\n",
        "\n",
        "    # Split data\n",
        "    train_series = series[series.index <= '2010-12-31']\n",
        "    val_series = series[series.index >= '2011-01-01']\n",
        "\n",
        "    # Get parameter ranges from config\n",
        "    p_range = range(0, 3)\n",
        "    d_range = config.get('d', range(1, 2))\n",
        "    q_range = config.get('q', range(0, 3))\n",
        "    P_range = range(0, 2)\n",
        "    D_range = range(0, 2)\n",
        "    Q_range = range(0, 2)\n",
        "\n",
        "    pdq = list(itertools.product(p_range, d_range, q_range))\n",
        "    seasonal_pdq = list(itertools.product(P_range, D_range, Q_range, [12]))\n",
        "\n",
        "    # Grid search\n",
        "    best_metric = np.inf\n",
        "    best_params = None\n",
        "\n",
        "    for param in pdq:\n",
        "        for seasonal_param in seasonal_pdq:\n",
        "            try:\n",
        "                model = SARIMAX(\n",
        "                    train_series,\n",
        "                    order=param,\n",
        "                    seasonal_order=seasonal_param,\n",
        "                    enforce_stationarity=False,\n",
        "                    enforce_invertibility=False\n",
        "                )\n",
        "                results = model.fit(disp=False, maxiter=200)\n",
        "\n",
        "                # Validate\n",
        "                val_forecast = results.get_forecast(steps=len(val_series))\n",
        "                val_pred = val_forecast.predicted_mean\n",
        "\n",
        "                # Revert transformation for error calculation\n",
        "                if config['transform'] == 'log':\n",
        "                    val_pred_orig = np.expm1(val_pred)\n",
        "                    val_actual_orig = np.expm1(val_series)\n",
        "                else:\n",
        "                    val_pred_orig = val_pred\n",
        "                    val_actual_orig = val_series\n",
        "\n",
        "                # Select error metric\n",
        "                if config['error_metric'] == 'smape':\n",
        "                    error = smape(val_actual_orig, val_pred_orig)\n",
        "                else:\n",
        "                    error = mean_absolute_error(val_actual_orig, val_pred_orig)\n",
        "\n",
        "                if error < best_metric:\n",
        "                    best_metric = error\n",
        "                    best_params = (param, seasonal_param)\n",
        "\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    if best_params is None:\n",
        "        best_params = ((1,1,1), (1,1,1,12))\n",
        "        print(\"Using fallback parameters\")\n",
        "\n",
        "    print(f\"Best SARIMA{best_params[0]}{best_params[1]} with {config['error_metric'].upper()}: {best_metric:.2f}\")\n",
        "\n",
        "    # Final model with best params\n",
        "    model = SARIMAX(\n",
        "        series,\n",
        "        order=best_params[0],\n",
        "        seasonal_order=best_params[1]\n",
        "    )\n",
        "    final_model = model.fit(disp=False)\n",
        "\n",
        "    # Forecast\n",
        "    test_forecast = final_model.get_forecast(steps=18)\n",
        "    forecast = test_forecast.predicted_mean\n",
        "\n",
        "    # Revert transformation\n",
        "    if config['transform'] == 'log':\n",
        "        forecast = np.expm1(forecast)\n",
        "\n",
        "    crime_forecasts[crime_type] = forecast\n",
        "    validation_metrics[crime_type] = best_metric\n",
        "\n",
        "# Generate submission\n",
        "forecast_dates = pd.date_range(start='2012-01-01', periods=18, freq='MS')\n",
        "submission_dfs = []\n",
        "\n",
        "for crime_type in all_crimes:\n",
        "    if crime_type in crime_forecasts:\n",
        "        forecast = crime_forecasts[crime_type]\n",
        "        crime_df = pd.DataFrame({\n",
        "            'YEAR': forecast_dates.year,\n",
        "            'MONTH': forecast_dates.month,\n",
        "            'TYPE': crime_type,\n",
        "            'Incident_Counts': forecast.values.round(2)\n",
        "        })\n",
        "    else:\n",
        "        # Handle missing crime type (Offence Against a Person)\n",
        "        crime_df = pd.DataFrame({\n",
        "            'YEAR': forecast_dates.year,\n",
        "            'MONTH': forecast_dates.month,\n",
        "            'TYPE': crime_type,\n",
        "            'Incident_Counts': 0.0\n",
        "        })\n",
        "    submission_dfs.append(crime_df)\n",
        "\n",
        "submission = pd.concat(submission_dfs)\n",
        "submission = submission.sort_values(['YEAR', 'MONTH', 'TYPE']).reset_index(drop=True)\n",
        "submission.to_csv('fbi_crime_forecast_optimized.csv', index=False)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I implemented a Seasonal AutoRegressive Integrated Moving Average (SARIMA) model for each crime type, with crime-specific optimizations:\n",
        "'''General Model Structure:\n",
        "SARIMA(p, d, q)(P, D, Q, 12) where:\n",
        "\n",
        "p: AutoRegressive order (temporal dependencies)\n",
        "\n",
        "d: Differencing order (trend removal)\n",
        "\n",
        "q: Moving Average order (error correction)\n",
        "\n",
        "P, D, Q: Seasonal equivalents\n",
        "\n",
        "12: Monthly seasonality'''\n",
        "# I had to use Log transformation for high-variance crimes (Theft from Vehicle, Theft of Bicycle)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "\n",
        "# Data from results\n",
        "crimes = ['B&E Comm', 'B&E Res', 'Mischief', 'Other Theft',\n",
        "          'Veh Theft', 'Bike Theft', 'Veh Collision']\n",
        "metrics = [14.16, 27.31, 22.20, 18.31, 11.96, 10.54, 9.67]  # sMAPE for last 3 converted to MAE equivalents\n",
        "\n",
        "# Create visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(crimes, metrics, color=['#1f77b4' if m < 20 else '#ff7f0e' for m in metrics])\n",
        "\n",
        "# Annotate metric types\n",
        "for i, bar in enumerate(bars):\n",
        "    metric_type = 'MAE' if i < 4 else 'sMAPE (%)'\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.5,\n",
        "            f'{metrics[i]}\\n({metric_type})',\n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "ax.set_title('Crime Forecasting Performance by Type', fontsize=14)\n",
        "ax.set_ylabel('Error Metric Value', fontsize=12)\n",
        "ax.set_ylim(0, 35)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Interpretation box\n",
        "textstr = ('Key Insights:\\n'\n",
        "           '- Vehicle crimes: <12% sMAPE\\n'\n",
        "           '- Property crimes: <20 MAE\\n'\n",
        "           '- Collisions: Near-perfect prediction')\n",
        "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        "ax.text(0.95, 0.95, textstr, transform=ax.transAxes,\n",
        "        fontsize=10, va='top', ha='right', bbox=props)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "#I directly integrated hyperparameter tuning with the model in Model Implementation stage because of time restriction\n",
        "#of this project and for achieving  optimal results."
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I employed a `Grid Search` approach for hyperparameter optimization.\n",
        "\n",
        "**Why Grid Search:**\n",
        "\n",
        "1.  SARIMA parameters (`p, d, q, P, D, Q`). Grid Search exhaustively tries every possible combination within this predefined grid. This ensures that I find the optimal parameter combination *within that grid*.\n",
        "2. Unlike techniques like Random Search or Bayesian Optimization, Grid Search provides direct control over the parameter space being explored. For SARIMA models, understanding the typical ranges and constraints of the parameters (e.g., `d` and `D` often being 0 or 1 for stationarity, seasonal period being 12 for monthly data) makes a defined grid a sensible approach.\n",
        "3. This implementation allowed for defining specific parameter grids (`d`, `q`) and error metrics (`mae`, `smape`) for different crime types (like 'Theft from Vehicle' and 'Theft of Bicycle') based on their observed characteristics (high variance, different seasonality needs). Grid search facilitates this kind of targeted tuning.\n",
        "\n",
        "Grid Search provides a reliable way to find the best parameters within a well-defined search window for each individual time series."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `Mean Absolute Error` (MAE):\n",
        "- MAE is used as the default error metric for most crime types.\n",
        "- MAE is highly intuitive and easy to understand. It represents the average magnitude of errors in a set of forecasts.\n",
        "- Since the model forecasts 'Incident_Counts', MAE directly reflects the average absolute difference between the predicted number of incidents and the actual number.\n",
        "- MAE helps in planning and allocating resources (e.g., police patrols, investigative units, community programs). If the MAE for 'Break and Enter Commercial' is 14.16, it means on average, the forecast is off by about 14 incidents.\n",
        "2. `Symmetric Mean Absolute Percentage Error` (sMAPE):\n",
        "- sMAPE is specifically used for 'Theft from Vehicle' and 'Theft of Bicycle'.\n",
        "- Many business stakeholders are accustomed to thinking in terms of percentages. sMAPE provides this kind of intuitive understanding of forecasting accuracy, which can be very impactful for high-level strategic discussions and performance reviews, especially for \"vehicle crimes\" as highlighted in the key insights."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I chose `SARIMA` model as my final prediction model because for the specific FBI Crime data provided there were high variance in specific crime types (`Theft from Vehicle` and` Theft of Bicycle`), SARIMA can be optimized for such issues in the training data.\n",
        "- Log transformation was also applied to the above mentioned crime types to reduce the average error in the model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Perform model and feature importance explainability\n",
        "\n",
        "# Since SARIMA is a statistical time series model, it doesn't inherently have feature importance in the same way tree-based or linear regression models do.\n",
        "# However, we can analyze the fitted model's components and coefficients to understand the importance of different time series patterns.\n",
        "\n",
        "# The variable 'final_model_results' is a summary object, not a dictionary of model results.\n",
        "# We should directly use the fitted model object 'final_model' to get the summary and fitted values.\n",
        "\n",
        "print(f\"\\n===== Model Explainability for {crime_type_to_explain} (SARIMA) =====\")\n",
        "\n",
        "# Display the model summary\n",
        "print(final_model.summary())\n",
        "\n",
        "# We need the configuration (like transformation used) for the specific crime type\n",
        "# Let's assume 'crime_config' dictionary is available and contains config for each crime type.\n",
        "# If the final_model was fitted on a specific crime type, we'll use its config.\n",
        "# If it was fitted on an aggregate series, you might need to adjust.\n",
        "# For now, we use the config for the crime_type_to_explain.\n",
        "config = crime_config.get(crime_type_to_explain, crime_config['default'])\n",
        "\n",
        "\n",
        "print(\"\\nIn a SARIMA model:\")\n",
        "print(\"- p (AR order): Significance indicates dependence on past values.\")\n",
        "print(\"- d (Differencing order): Higher order indicates a stronger trend.\")\n",
        "print(\"- q (MA order): Significance indicates dependence on past forecast errors.\")\n",
        "print(\"- P (Seasonal AR order): Significance indicates dependence on past seasonal values (12 months ago).\")\n",
        "print(\"- D (Seasonal Differencing order): Higher order indicates a stronger seasonal trend.\")\n",
        "print(\"- Q (Seasonal MA order): Significance indicates dependence on past seasonal forecast errors.\")\n",
        "\n",
        "print(\"\\nAnalyzing the summary table:\")\n",
        "print(\"- Look at the 'P>|z|' column for the coefficients (e.g., ar.L1, ma.L1, ar.S.L12, ma.S.L12).\")\n",
        "print(\"- Small p-values (typically < 0.05) indicate that the corresponding parameter is statistically significant.\")\n",
        "print(\"- Significant non-seasonal terms (ar.L#, ma.L#) indicate the importance of recent past.\")\n",
        "print(\"- Significant seasonal terms (ar.S.L12, ma.S.L12) indicate the importance of values from the same month in previous years.\")\n",
        "\n",
        "# Access fitted values directly from the fitted model object\n",
        "fitted_values = final_model.fittedvalues\n",
        "\n",
        "# We need the original raw series to plot against the fitted values.\n",
        "# Assuming 'raw_series' is the time series data the model was fitted on.\n",
        "# If the model was fitted on a transformed series, we need to reverse the transformation for plotting.\n",
        "raw_series_crime = raw_series.copy() # Assuming raw_series holds the data for the fitted model\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "# Reverse the transformation if used, for plotting\n",
        "if config.get('transform') == 'log':\n",
        "    plt.plot(raw_series_crime.index, raw_series_crime.values, label='Actual Counts', alpha=0.7)\n",
        "    plt.plot(fitted_values.index, np.expm1(fitted_values.values), label='Fitted Values (Inverse Log)', linestyle='--')\n",
        "else:\n",
        "    plt.plot(raw_series_crime.index, raw_series_crime.values, label='Actual Counts', alpha=0.7)\n",
        "    plt.plot(fitted_values.index, fitted_values.values, label='Fitted Values', linestyle='--')\n",
        "\n",
        "\n",
        "plt.title(f'Actual vs. Fitted Incident Counts for {crime_type_to_explain}') # Title based on the specified crime type\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Incident Counts')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Note: The interpretation of feature importance is done by looking at the p-values in the summary table.\n",
        "# Significant parameters indicate which past patterns (AR, MA, Seasonal AR, Seasonal MA) are important."
      ],
      "metadata": {
        "id": "b5t4DSPnMrWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Conclusion: FBI Crime Time Series Forecasting\n",
        "\n",
        "- This project successfully developed and evaluated time series forecasting models for FBI crime data. The analysis revealed significant trends, seasonality, and correlations within the dataset.\n",
        "\n",
        "- Data preprocessing involved cleaning, handling duplicates, addressing invalid spatial data, and aggregating individual incidents into monthly crime counts by type. This transformation was crucial for creating time series suitable for analysis. Exploratory data analysis identified a general downward trend in total crime from 1999 to 2011, with pronounced seasonal peaks in summer months (Q2/Q3).\n",
        "\n",
        "- Hypothesis testing statistically confirmed these observations: a significant difference in crime rates between summer and winter months, a structural break in the overall crime trend around 2009, and a significant positive correlation between \"Theft from Vehicle\" and \"Theft of Vehicle\". Outlier treatment using Winsorization, Robust Regression, and Mahalanobis Distance was applied where indicated by hypothesis testing results to improve data robustness.\n",
        "\n",
        "- The core of the forecasting involved implementing SARIMA models for each crime type. Recognizing the unique characteristics of different crime categories, specific optimizations were applied, including logarithmic transformations for high-variance series like `Theft from Vehicle` and `Theft of Bicycle`, and targeted parameter `grid searches` during hyperparameter tuning. Grid search was chosen for its systematic exploration of the defined parameter space, ensuring that optimal SARIMA configurations were found for each time series within the grid.\n",
        "\n",
        "- The SARIMA models demonstrated reasonable performance based on MAE and sMAPE metrics on the validation set (2011 data). Metrics varied by crime type, reflecting the inherent predictability of each series. Overall, the models successfully captured the historical patterns and provided forecasts for the 2012-2013 period.\n",
        "\n",
        "- `Future work` could explore more advanced time series models, such as Prophet, Exponential Smoothing variations.\n",
        "\n",
        "- In summary, this project provides a solid foundation for understanding and forecasting FBI crime trends using time series methods. The insights gained from the analysis and the implemented SARIMA models offer valuable tools for informing resource allocation and strategic planning for crime prevention and response."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}